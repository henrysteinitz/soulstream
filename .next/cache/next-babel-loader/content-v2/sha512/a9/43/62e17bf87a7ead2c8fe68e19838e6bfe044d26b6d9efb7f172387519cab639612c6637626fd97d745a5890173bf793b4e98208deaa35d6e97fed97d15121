{"ast":null,"code":"import _classCallCheck from \"/Users/henrysteinitz/Desktop/soulstream/node_modules/@babel/runtime/helpers/esm/classCallCheck\";\nimport _createClass from \"/Users/henrysteinitz/Desktop/soulstream/node_modules/@babel/runtime/helpers/esm/createClass\";\n\nvar Voice = /*#__PURE__*/function () {\n  function Voice() {\n    _classCallCheck(this, Voice);\n\n    this.synth = window.speechSynthesis;\n    this.context = new (window.AudioContext || window.webkitAudioContext)();\n    this.bufferSource = this.context.createBufferSource();\n    var channelSplitter = this.context.createChannelSplitter(4);\n    var delay1 = this.context.createDelay(1.0);\n    var delay2 = this.context.createDelay(2.0);\n    var delay3 = this.context.createDelay(3.0);\n    var delay4 = this.context.createDelay(4.0);\n    channelSplitter.connect(delay1, 0, 0);\n    channelSplitter.connect(delay1, 1, 0);\n    channelSplitter.connect(delay1, 2, 0);\n    channelSplitter.connect(delay1, 3, 0);\n    var channelMerger = this.context.createChannelMerger(4);\n    delay1.connect(channelMerger, 0, 0);\n    delay2.connect(channelMerger, 0, 1);\n    delay3.connect(channelMerger, 0, 2);\n    delay4.connect(channelMerger, 0, 3);\n    channelMerger.connect(this.context.destination);\n  }\n\n  _createClass(Voice, [{\n    key: \"say\",\n    value: function say(script) {\n      var _this = this;\n\n      if (navigator.mediaDevices && navigator.mediaDevices.getUserMedia) {\n        console.log('getUserMedia supported.');\n        navigator.mediaDevices.getUserMedia({\n          audio: true\n        }).then(function (stream) {\n          var mediaRecorder = new MediaRecorder(stream);\n          var utterance = new SpeechSynthesisUtterance(script);\n\n          utterance.onend = function () {\n            mediaRecorder.stop();\n\n            mediaRecorder.ondataavailable = function (e) {\n              e.data.arrayBuffer().then(function (buffer) {\n                console.log(buffer);\n\n                _this.context.decodeAudioData(buffer, function (audioBuffer) {\n                  _this.bufferSource.buffer = audioBuffer;\n\n                  _this.bufferSource.start();\n                });\n              });\n            };\n          };\n\n          mediaRecorder.start();\n\n          _this.synth.speak(utterance);\n        });\n      }\n    }\n  }, {\n    key: \"loop\",\n    value: function loop() {\n      if (!window) {\n        return;\n      }\n\n      return;\n    }\n  }, {\n    key: \"save\",\n    value: function save() {\n      if (!window) {\n        return;\n      }\n\n      return;\n    }\n  }]);\n\n  return Voice;\n}();\n\nexport { Voice as default };","map":{"version":3,"sources":["/Users/henrysteinitz/Desktop/soulstream/lib/voice/voice.js"],"names":["Voice","synth","window","speechSynthesis","context","AudioContext","webkitAudioContext","bufferSource","createBufferSource","channelSplitter","createChannelSplitter","delay1","createDelay","delay2","delay3","delay4","connect","channelMerger","createChannelMerger","destination","script","navigator","mediaDevices","getUserMedia","console","log","audio","then","stream","mediaRecorder","MediaRecorder","utterance","SpeechSynthesisUtterance","onend","stop","ondataavailable","e","data","arrayBuffer","buffer","decodeAudioData","audioBuffer","start","speak"],"mappings":";;;IAAqBA,K;AAEpB,mBAAc;AAAA;;AACb,SAAKC,KAAL,GAAaC,MAAM,CAACC,eAApB;AACA,SAAKC,OAAL,GAAe,KAAKF,MAAM,CAACG,YAAP,IAAuBH,MAAM,CAACI,kBAAnC,GAAf;AAEA,SAAKC,YAAL,GAAoB,KAAKH,OAAL,CAAaI,kBAAb,EAApB;AACA,QAAMC,eAAe,GAAG,KAAKL,OAAL,CAAaM,qBAAb,CAAmC,CAAnC,CAAxB;AACA,QAAMC,MAAM,GAAG,KAAKP,OAAL,CAAaQ,WAAb,CAAyB,GAAzB,CAAf;AACA,QAAMC,MAAM,GAAG,KAAKT,OAAL,CAAaQ,WAAb,CAAyB,GAAzB,CAAf;AACA,QAAME,MAAM,GAAG,KAAKV,OAAL,CAAaQ,WAAb,CAAyB,GAAzB,CAAf;AACA,QAAMG,MAAM,GAAG,KAAKX,OAAL,CAAaQ,WAAb,CAAyB,GAAzB,CAAf;AACAH,IAAAA,eAAe,CAACO,OAAhB,CAAwBL,MAAxB,EAAgC,CAAhC,EAAmC,CAAnC;AACAF,IAAAA,eAAe,CAACO,OAAhB,CAAwBL,MAAxB,EAAgC,CAAhC,EAAmC,CAAnC;AACAF,IAAAA,eAAe,CAACO,OAAhB,CAAwBL,MAAxB,EAAgC,CAAhC,EAAmC,CAAnC;AACAF,IAAAA,eAAe,CAACO,OAAhB,CAAwBL,MAAxB,EAAgC,CAAhC,EAAmC,CAAnC;AACA,QAAMM,aAAa,GAAG,KAAKb,OAAL,CAAac,mBAAb,CAAiC,CAAjC,CAAtB;AACAP,IAAAA,MAAM,CAACK,OAAP,CAAeC,aAAf,EAA8B,CAA9B,EAAiC,CAAjC;AACAJ,IAAAA,MAAM,CAACG,OAAP,CAAeC,aAAf,EAA8B,CAA9B,EAAiC,CAAjC;AACAH,IAAAA,MAAM,CAACE,OAAP,CAAeC,aAAf,EAA8B,CAA9B,EAAiC,CAAjC;AACAF,IAAAA,MAAM,CAACC,OAAP,CAAeC,aAAf,EAA8B,CAA9B,EAAiC,CAAjC;AACAA,IAAAA,aAAa,CAACD,OAAd,CAAsB,KAAKZ,OAAL,CAAae,WAAnC;AACA;;;;wBAEGC,M,EAAQ;AAAA;;AACX,UAAIC,SAAS,CAACC,YAAV,IAA0BD,SAAS,CAACC,YAAV,CAAuBC,YAArD,EAAmE;AAC/DC,QAAAA,OAAO,CAACC,GAAR,CAAY,yBAAZ;AACAJ,QAAAA,SAAS,CAACC,YAAV,CAAuBC,YAAvB,CAAqC;AAAEG,UAAAA,KAAK,EAAE;AAAT,SAArC,EACGC,IADH,CACQ,UAACC,MAAD,EAAY;AACjB,cAAMC,aAAa,GAAG,IAAIC,aAAJ,CAAkBF,MAAlB,CAAtB;AACA,cAAMG,SAAS,GAAG,IAAIC,wBAAJ,CAA6BZ,MAA7B,CAAlB;;AACAW,UAAAA,SAAS,CAACE,KAAV,GAAkB,YAAM;AACvBJ,YAAAA,aAAa,CAACK,IAAd;;AACAL,YAAAA,aAAa,CAACM,eAAd,GAAgC,UAACC,CAAD,EAAO;AACtCA,cAAAA,CAAC,CAACC,IAAF,CAAOC,WAAP,GAAqBX,IAArB,CAA0B,UAACY,MAAD,EAAY;AACrCf,gBAAAA,OAAO,CAACC,GAAR,CAAYc,MAAZ;;AACA,gBAAA,KAAI,CAACnC,OAAL,CAAaoC,eAAb,CAA6BD,MAA7B,EAAqC,UAACE,WAAD,EAAiB;AACrD,kBAAA,KAAI,CAAClC,YAAL,CAAkBgC,MAAlB,GAA2BE,WAA3B;;AACA,kBAAA,KAAI,CAAClC,YAAL,CAAkBmC,KAAlB;AACA,iBAHD;AAIA,eAND;AAOA,aARD;AASA,WAXD;;AAaAb,UAAAA,aAAa,CAACa,KAAd;;AACA,UAAA,KAAI,CAACzC,KAAL,CAAW0C,KAAX,CAAiBZ,SAAjB;AACA,SAnBH;AAqBE;AACN;;;2BAEM;AACN,UAAI,CAAC7B,MAAL,EAAa;AACZ;AACA;;AACD;AACA;;;2BAEM;AACN,UAAI,CAACA,MAAL,EAAa;AACZ;AACA;;AACD;AACA;;;;;;SA/DmBF,K","sourcesContent":["export default class Voice {\n\n\tconstructor() {\n\t\tthis.synth = window.speechSynthesis\n\t\tthis.context = new (window.AudioContext || window.webkitAudioContext)()\n\n\t\tthis.bufferSource = this.context.createBufferSource()\n\t\tconst channelSplitter = this.context.createChannelSplitter(4)\n\t\tconst delay1 = this.context.createDelay(1.0)\n\t\tconst delay2 = this.context.createDelay(2.0)\n\t\tconst delay3 = this.context.createDelay(3.0)\n\t\tconst delay4 = this.context.createDelay(4.0)\n\t\tchannelSplitter.connect(delay1, 0, 0)\n\t\tchannelSplitter.connect(delay1, 1, 0)\n\t\tchannelSplitter.connect(delay1, 2, 0)\n\t\tchannelSplitter.connect(delay1, 3, 0)\n\t\tconst channelMerger = this.context.createChannelMerger(4)\n\t\tdelay1.connect(channelMerger, 0, 0)\n\t\tdelay2.connect(channelMerger, 0, 1)\n\t\tdelay3.connect(channelMerger, 0, 2)\n\t\tdelay4.connect(channelMerger, 0, 3)\n\t\tchannelMerger.connect(this.context.destination)\n\t}\n\n\tsay(script) {\n\t\tif (navigator.mediaDevices && navigator.mediaDevices.getUserMedia) {\n   \t\t\tconsole.log('getUserMedia supported.');\n   \t\t\tnavigator.mediaDevices.getUserMedia ({ audio: true })\n      \t\t.then((stream) => {\n      \t\t\tconst mediaRecorder = new MediaRecorder(stream);\n      \t\t\tconst utterance = new SpeechSynthesisUtterance(script);\n      \t\t\tutterance.onend = () => {\n      \t\t\t\tmediaRecorder.stop()\n      \t\t\t\tmediaRecorder.ondataavailable = (e) => {\n\t      \t\t\t\te.data.arrayBuffer().then((buffer) => {\n\t      \t\t\t\t\tconsole.log(buffer)\n\t      \t\t\t\t\tthis.context.decodeAudioData(buffer, (audioBuffer) => {\n\t      \t\t\t\t\t\tthis.bufferSource.buffer = audioBuffer\n\t      \t\t\t\t\t\tthis.bufferSource.start()\n\t      \t\t\t\t\t})\n\t      \t\t\t\t})\n      \t\t\t\t}\n      \t\t\t}\n\n      \t\t\tmediaRecorder.start()\n      \t\t\tthis.synth.speak(utterance)\n      \t\t})\n\n      \t}\n\t}\n\n\tloop() {\n\t\tif (!window) {\n\t\t\treturn\n\t\t}\n\t\treturn\n\t}\n\n\tsave() {\n\t\tif (!window) {\n\t\t\treturn\n\t\t}\n\t\treturn\n\t}\n\n}\n"]},"metadata":{},"sourceType":"module"}